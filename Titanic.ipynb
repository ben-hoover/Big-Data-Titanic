{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Predicting the Survival of Passengers on the RMS <i>Titanic</i> Using Demographic Data</h1>\n",
    "<br>\n",
    "<b>Benjamin Hoover</b>\n",
    "<br>\n",
    "<img src=\"titanic.jpg\" width=\"600\" height=\"450\" align=\"left\"></img>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "This lab determines the best machine learning model (DecisionTree, Random Forest, Nearest Neighbors) for data about Titanic passengers, and then uses it to determine what combinations of sex, age (children or adult), and class were most influential in determining a passenger's survival and what combinations most likely result in survival or death."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#code to import a csv and convert it to a numpy array\n",
    "#import the numpy library\n",
    "import numpy as np\n",
    "import statistics\n",
    "#open the file\n",
    "raw_data = open(\"titanic.csv\")\n",
    "# load the CSV file as a numpy matrix\n",
    "dataset = np.loadtxt(raw_data, delimiter=\",\")\n",
    "#'shape' is the dimensions of the matrix\n",
    "# separate the data from the target attributes\n",
    "X = dataset[:,0:3]\n",
    "y = dataset[:,3]\n",
    "\n",
    "\n",
    "tests = 1000\n",
    "\n",
    "#TITANIC.csv = Class, Age, Sex, Survived\n",
    "#reordered csv: Class, Sex, Age, Survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Part 1: Which machine learning algorithm - DecisionTree, KNeighbors, or RandomForest is most accurate for the dataset? What are their respective accuracies?</h3>\n",
    "\n",
    "This part determines the best algorithm and each's accuracy using 1000 tests of a random test data set consisting of 25% of the total data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree has an accuracy of 78.7869328494%\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "count = 0;\n",
    "avg = []\n",
    "while (count < tests): \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25)\n",
    "    my_classifier = tree.DecisionTreeClassifier();\n",
    "    my_classifier = my_classifier.fit(X_train, y_train)\n",
    "    predictions = my_classifier.predict(X_test)\n",
    "    avg.append(accuracy_score(predictions, y_test))\n",
    "    count += 1\n",
    "print(\"DecisionTree has an accuracy of\", str(statistics.mean(avg)*100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighbors has an accuracy of 75.8215970962%\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "count = 0;\n",
    "avg = []\n",
    "while (count < tests): \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25)\n",
    "    my_classifier = neighbors.KNeighborsClassifier();\n",
    "    my_classifier = my_classifier.fit(X_train, y_train)\n",
    "    predictions = my_classifier.predict(X_test)\n",
    "    avg.append(accuracy_score(predictions, y_test))\n",
    "    count += 1\n",
    "print(\"KNeighbors has an accuracy of\", str(statistics.mean(avg) * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest has an accuracy of 78.7362976407%\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "count = 0;\n",
    "avg = []\n",
    "while (count < tests): \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25)\n",
    "    my_classifier = ensemble.RandomForestClassifier();\n",
    "    my_classifier = my_classifier.fit(X_train, y_train)\n",
    "    predictions = my_classifier.predict(X_test)\n",
    "    avg.append(accuracy_score(predictions, y_test))\n",
    "    count += 1\n",
    "print(\"RandomForest has an accuracy of\", str(statistics.mean(avg) * 100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Based off of the average of 1000 tests, the Decision Tree Classifier is the best, being slightly more accurate than the Random Forest and significantly better than KNearestNeighbors.\n",
    "\n",
    "Note that due to random variation, sometimes RandomForest will appear more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<h3>Part 2: Using the most accurate algorithm, who would have survived?</h3>\n",
    "\n",
    "This part uses the best algorithm (Decision Tree) to determine whether the following imaginary people would have survived:\n",
    "\n",
    "a. An adult male in the 3rd class\n",
    "\n",
    "b. An adult female in 1st class\n",
    "\n",
    "c. A female child in 1st class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.]\n",
      "An adult male in the 3rd class would have most likely died.\n"
     ]
    }
   ],
   "source": [
    "my_classifier = tree.DecisionTreeClassifier();\n",
    "my_classifier = my_classifier.fit(X, y)\n",
    "print(my_classifier.predict([[3 ,1, 1]]))\n",
    "print(\"An adult male in the 3rd class would have most likely died.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.]\n",
      "An adult female in 1st class would have most likely survived.\n"
     ]
    }
   ],
   "source": [
    "print(my_classifier.predict([[1 ,1, 0]]))\n",
    "print(\"An adult female in 1st class would have most likely survived.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.]\n",
      "A female child in 1st class would have most likely survived.\n"
     ]
    }
   ],
   "source": [
    "print(my_classifier.predict([[1 ,0, 0]]))\n",
    "print(\"A female child in 1st class would have most likely survived.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, the adult male in 3rd class would have most likely died, while the 1st class female adult and child would have most likely survived. These results seem unsurprising, considering that \"women and children\" were supposedly prioritized for spots in lifeboats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Part 3: What is the most important determining feature in the dataset?</h3>\n",
    "\n",
    "This part of the lab determines the most influential feature in the dataset. It does this by getting the accuracy of the algorithm's predictions using only 1 or 2 of the features. The feature with the highest accuracy is the most influential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#control test\n",
    "resultdict = {}\n",
    "X = dataset[:,0:3]\n",
    "y = dataset[:,3]\n",
    "count = 0;\n",
    "avg = []\n",
    "while (count < tests): \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25)\n",
    "    my_classifier = tree.DecisionTreeClassifier();\n",
    "    my_classifier = my_classifier.fit(X_train, y_train)\n",
    "    predictions = my_classifier.predict(X_test)\n",
    "    avg.append(accuracy_score(predictions, y_test))\n",
    "    count += 1\n",
    "avgFinal = statistics.mean(avg)\n",
    "resultdict[\"control\"] = avgFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#class and age\n",
    "X = dataset[:,0:2]\n",
    "y = dataset[:,3]\n",
    "count = 0;\n",
    "avg = []\n",
    "while (count < tests): \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25)\n",
    "    my_classifier = tree.DecisionTreeClassifier();\n",
    "    my_classifier = my_classifier.fit(X_train, y_train)\n",
    "    predictions = my_classifier.predict(X_test)\n",
    "    avg.append(accuracy_score(predictions, y_test))\n",
    "    count += 1\n",
    "avgFinal = statistics.mean(avg)\n",
    "resultdict[\"class and age\"] = avgFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#age and sex\n",
    "X = dataset[:,1:3]\n",
    "y = dataset[:,3]\n",
    "count = 0;\n",
    "avg = []\n",
    "while (count < tests): \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25)\n",
    "    my_classifier = tree.DecisionTreeClassifier();\n",
    "    my_classifier = my_classifier.fit(X_train, y_train)\n",
    "    predictions = my_classifier.predict(X_test)\n",
    "    avg.append(accuracy_score(predictions, y_test))\n",
    "    count += 1\n",
    "avgFinal = statistics.mean(avg)\n",
    "resultdict[\"age and sex\"] = avgFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reordered csv: Class, Sex, Age, Survived\n",
    "raw_data2 = open(\"titanic2.csv\")\n",
    "# load the CSV file as a numpy matrix\n",
    "dataset2 = np.loadtxt(raw_data2, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#class and sex\n",
    "X = dataset2[:,0:2]\n",
    "y = dataset2[:,3]\n",
    "count = 0;\n",
    "avg = []\n",
    "while (count < tests): \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25)\n",
    "    my_classifier = tree.DecisionTreeClassifier();\n",
    "    my_classifier = my_classifier.fit(X_train, y_train)\n",
    "    predictions = my_classifier.predict(X_test)\n",
    "    avg.append(accuracy_score(predictions, y_test))\n",
    "    count += 1\n",
    "avgFinal = statistics.mean(avg)\n",
    "resultdict[\"class and sex\"] = avgFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Class\n",
    "#TITANIC.csv = Class, Age, Sex, Survived\n",
    "X = dataset[:,0:1]\n",
    "y = dataset[:,3]\n",
    "count = 0;\n",
    "avg = []\n",
    "while (count < tests): \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25)\n",
    "    my_classifier = tree.DecisionTreeClassifier();\n",
    "    my_classifier = my_classifier.fit(X_train, y_train)\n",
    "    predictions = my_classifier.predict(X_test)\n",
    "    avg.append(accuracy_score(predictions, y_test))\n",
    "    count += 1\n",
    "avgFinal = statistics.mean(avg)\n",
    "resultdict[\"class\"] = avgFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#age\n",
    "X = dataset[:,1:2]\n",
    "y = dataset[:,3]\n",
    "count = 0;\n",
    "avg = []\n",
    "while (count < tests): \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25)\n",
    "    my_classifier = tree.DecisionTreeClassifier();\n",
    "    my_classifier = my_classifier.fit(X_train, y_train)\n",
    "    predictions = my_classifier.predict(X_test)\n",
    "    avg.append(accuracy_score(predictions, y_test))\n",
    "    count += 1\n",
    "avgFinal = statistics.mean(avg)\n",
    "resultdict[\"age\"] = avgFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#age\n",
    "X = dataset[:,2:3]\n",
    "y = dataset[:,3]\n",
    "count = 0;\n",
    "avg = []\n",
    "while (count < tests): \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25)\n",
    "    my_classifier = tree.DecisionTreeClassifier();\n",
    "    my_classifier = my_classifier.fit(X_train, y_train)\n",
    "    predictions = my_classifier.predict(X_test)\n",
    "    avg.append(accuracy_score(predictions, y_test))\n",
    "    count += 1\n",
    "avgFinal = statistics.mean(avg)\n",
    "resultdict[\"sex\"] = avgFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy based on the following demogrpahics: {'sex': '77.6773139746%', 'age and sex': '77.3557168784%', 'class and age': '72.4578947368%', 'class and sex': '78.2878402904%', 'control': '78.8711433757%', 'age': '67.6588021779%', 'class': '71.3626134301%'}\n"
     ]
    }
   ],
   "source": [
    "for resultkey in resultdict.keys():\n",
    "    resultdict[resultkey] = str(resultdict[resultkey] * 100) + \"%\"\n",
    "print(\"Accuracy based on the following demogrpahics:\", resultdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As visible by the accuracy percents, sex proved to be the best indicator of a passenger's survival, with an accuracy percent of 77.7% besides all 3 demographics, which had an accuracy percent of 78.9%. The class was the 2nd most important metric, with an accuracy percentage of 71.4%, followed by age, with an accuracy percentage of 67.7%. The most effective combination of metrics was class and sex, with an accuracy percent of 78.3%, which is unsurprising given that age was a much less important indicator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Conclusion</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, I found that the DecisionTree classifier was generally the most accurate classifier for this data set. I also found that sex was the most important influential feature in the dataset, followed by class, then age. I also found that an adult male in 3rd class would have most likely have died, while 1st class female adults and children were likely to have survived. These results make sense given the \"women and children\" prioritization in the evacuation, as well as the fact that sex was the most important determining feature. I was, however, surprised that age was the least important determining feature of the set considering children were supposedly prioritized. This could be because age only differentiated between children and adult, not between adults, and therefore was not a useful differentiator for the majority of passengers.\n",
    "\n",
    "I am wondering more about how age impacts survival - were older people a lower priority? The dataset only differentiates between adult and children, so these more in depth questions cannot be answered. I am also curious that if gender mattered for the children, or only for adults. Additionally, it would be interesting to see the survival rates for each group of people. I suspect that 1st class female children would have the highest survival rate, but it would be good to have data to verify this. While this additional information does not require machine learning, it would be helpful to get a better understanding of who survived on the Titanic."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
